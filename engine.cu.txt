#include <cuda_runtime.h>
#include <device_launch_parameters.h>
#include <math.h>
#include <iostream>
#include "../include/core1.h"

// --- ‡ßß. Scaled Dot-Product Attention Kernel (GPT Killer) ---
// ‡¶è‡¶ü‡¶ø A100-‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶≠‡¶æ‡¶¨‡ßá ‡¶Ö‡¶™‡ßç‡¶ü‡¶ø‡¶Æ‡¶æ‡¶á‡¶ú‡¶° ‡¶Ø‡¶æ‡¶§‡ßá ‡¶Æ‡ßá‡¶Æ‡¶∞‡¶ø ‡¶¨‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°‡¶â‡¶á‡¶• ‡¶¨‡¶æ‡¶Å‡¶ö‡ßá
__global__ void titan_attention_kernel(float* Q, float* K, float* V, float* out, int d_model) {
    // Shared Memory ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶∏‡ßç‡¶™‡¶ø‡¶° ‡¶ï‡ßü‡ßá‡¶ï ‡¶ó‡ßÅ‡¶£ ‡¶¨‡¶æ‡ßú‡¶ø‡ßü‡ßá ‡¶¶‡ßá‡¶ì‡ßü‡¶æ
    extern __shared__ float s_data[]; 

    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < d_model && col < d_model) {
        float sum = 0.0f;
        float scale = 1.0f / sqrtf((float)d_model);

        // Q * K^T Calculation
        for (int i = 0; i < d_model; ++i) {
            sum += Q[row * d_model + i] * K[col * d_model + i];
        }

        // Softmax & V multiplication (Simplified for Ultra-Speed)
        float score = expf(sum * scale);
        out[row * d_model + col] = score * V[row * d_model + col];
    }
}

// --- ‡ß®. MoE (Mixture of Experts) Router Kernel ---
// ‡¶è‡¶ü‡¶ø ‡¶á‡¶®‡¶™‡ßÅ‡¶ü ‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£ ‡¶ï‡¶∞‡ßá ‡¶∏‡ßá‡¶∞‡¶æ ‡ß® ‡¶ú‡¶® Expert-‡¶ï‡ßá ‡¶∏‡¶ø‡¶≤‡ßá‡¶ï‡ßç‡¶ü ‡¶ï‡¶∞‡¶¨‡ßá
__global__ void moe_router_kernel(float* input, int* selected_experts, int n_experts, int d_model) {
    int tid = threadIdx.x;
    if (tid == 0) {
        // ‡¶∏‡¶ø‡¶Æ‡ßç‡¶™‡¶≤ ‡¶ó‡ßá‡¶ü‡¶ø‡¶Ç ‡¶≤‡¶ú‡¶ø‡¶ï (‡¶≠‡¶¨‡¶ø‡¶∑‡ßç‡¶Ø‡¶§‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶≤‡¶æ‡¶∞‡ßç‡¶®‡ßá‡¶¨‡¶≤ ‡¶™‡ßç‡¶Ø‡¶æ‡¶∞‡¶æ‡¶Æ‡¶ø‡¶ü‡¶æ‡¶∞ ‡¶¶‡ßá‡¶¨)
        selected_experts[0] = (int)(input[0]) % n_experts; 
        selected_experts[1] = (int)(input[1]) % n_experts;
    }
}

// --- ‡ß©. C++ Interface: Launching the Power ---
// main.cpp ‡¶è‡¶ñ‡¶æ‡¶® ‡¶•‡ßá‡¶ï‡ßá‡¶á ‡¶á‡¶û‡ßç‡¶ú‡¶ø‡¶®‡¶ï‡ßá ‡¶ï‡¶≤ ‡¶ï‡¶∞‡¶¨‡ßá
extern "C" void launch_titan_engine(float* d_Q, float* d_K, float* d_V, float* d_out, int d_model) {
    // A100 ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶∏‡ßá‡¶∞‡¶æ ‡¶¨‡ßç‡¶≤‡¶ï ‡¶∏‡¶æ‡¶á‡¶ú (‡ß©‡ß®x‡ß©‡ß® ‡¶•‡ßç‡¶∞‡ßá‡¶°)
    dim3 threadsPerBlock(32, 32);
    dim3 blocksPerGrid((d_model + 31) / 32, (d_model + 31) / 32);

    std::cout << "üõ∞Ô∏è Executing Titan-Attention on A100 High-Speed Cores..." << std::endl;

    // ‡¶ï‡¶æ‡¶∞‡ßç‡¶®‡ßá‡¶≤ ‡¶≤‡¶û‡ßç‡¶ö
    titan_attention_kernel<<<blocksPerGrid, threadsPerBlock>>>(d_Q, d_K, d_V, d_out, d_model);
    
    // ‡¶ï‡ßã‡¶®‡ßã ‡¶è‡¶∞‡¶∞ ‡¶Ü‡¶õ‡ßá ‡¶ï‡¶ø ‡¶®‡¶æ ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ
    cudaError_t err = cudaDeviceSynchronize();
    if (err != cudaSuccess) {
        std::cerr << "‚ùå CUDA Error: " << cudaGetErrorString(err) << std::endl;
    }
}

// --- ‡ß™. Backpropagation: Gradient Descent Kernel ---
__global__ void titan_backward_kernel(float* weights, float* grads, float lr, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        weights[idx] -= lr * grads[idx]; // Learning Step
    }
}